---
layout: post
title: CUDA study 2-3. Parallel processing of CUDA data
subtitle: 
categories: CUDA
tags: [CUDA]
---

The entire process of parallel processing data using CUDA is as follows.

1. Allocate data to be used for input and output to PC memory.
2. Allocate data to be used for input and output to graphics memory.
3. Enter the value you want to process into PC memory.
4. Copy input data from PC memory to graphics memory.
5. Split the data and bring it to the GPU.
6. More than a thousand threads are created and processed in parallel using **kernel functions**.
7. Merge the processed results.
8. Transfer results to PC memory.
9. Free up graphics memory.
10. Free up PC memory.

The **kernel** refers to a function that processes operations in CUDA, such as the worker function in Windows multithreading programming.

The difference between worker functions and kernel functions lies in the process of creating threads. 

- **Worker functions** have a separate process for creating threads, 
- but CUDA programs also proceed with creating threads while calling **kernel functions**.